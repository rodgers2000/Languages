}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
x1 = 50
x2 = 50
learning_rate = .1
for (epoch in 1:10000) {
x1 = x1 - learning_rate * gradient_approx(func, x1, x1 + rnorm(1))
x2 = x2 - learning_rate * gradient_true(x2)
}
print(x1)
print(x2)
library(keras)
install_keras()
max_features = 10000
maxlen = 20
imdb = dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
View(x_test)
x_test[[1]]
x_test[[2]]
maxlen
x_train = pad_sequences(x_train, maxlen = maxlen)
x_test[[2]]
x_test[[1]]
x_train[[1]]
x_train[[2]]
View(x_train)
x_train = pad_sequences(x_train, maxlen = maxlen)
x_test = pad_sequences(x_test, maxlen = maxlen)
View(x_test)
model <- keras_model_sequential() %>%
layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 1, activation = "sigmoid")
summary(model)
dim(x_train)
y_train
summary(model)
View(x_test)
dim(x_train)
max_features = 10000
maxlen = 20
imdb = dataset_imdb(num_words = max_features)
View(imdb)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
View(x_test)
x_test[[3]]
x_train = pad_sequences(x_train, maxlen = maxlen)
View(x_test)
View(x_train)
x_test = pad_sequences(x_test, maxlen = maxlen)
View(x_test)
model <- keras_model_sequential() %>%
layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentrophy",
metrics = c("acc")
)
history = model %>% fit(
x_train, y_train,
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
model <- keras_model_sequential() %>%
layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentrophy",
metrics = c("acc")
)
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc")
)
history = model %>% fit(
x_train, y_train,
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
model <- keras_model_sequential() %>%
layer_embedding(input_dim = 1000, output_dim = 8, input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc")
)
history = model %>% fit(
x_train, y_train,
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
model <- keras_model_sequential() %>%
layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc")
)
history = model %>% fit(
x_train, y_train,
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
history = model %>% fit(
x_train, y_train,
epochs = 2,
batch_size = 32,
validation_split = 0.2
)
dim(x_train)
View(x_train)
summary(x_train)
View(x_test)
View(x_train)
summary(model)
model <- keras_model_sequential() %>%
# 10,000 words in the dictionary
layer_embedding(input_dim = 10000, output_dim = 80, input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc")
)
history = model %>% fit(
x_train, y_train,
epochs = 2,
batch_size = 32,
validation_split = 0.2
)
model <- keras_model_sequential() %>%
# 10,000 words in the dictionary
layer_embedding(input_dim = 10000, output_dim = 10, input_length = maxlen) %>%
layer_flatten() %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc")
)
history = model %>% fit(
x_train, y_train,
epochs = 2,
batch_size = 32,
validation_split = 0.2
)
x_train = array(data = 0, dim = c(1000, 30, 10)) # samples, timesteps, input_features
x_train
x_train[i, t, ] = rnorm(10)
x_train = array(data = 0, dim = c(1000, 30, 10)) # samples, timesteps, input_features
for (i in 1:1000) {
for (t in 1:30) {
x_train[i, t, ] = rnorm(10)
}
}
x_train = array(data = 0, dim = c(1000, 30, 10)) # samples, timesteps, input_features
for (i in 1:1000) {
for (t in 1:30) {
x_train[i, t, ] = rnorm(10)
}
}
x_train = array(data = 0, dim = c(1000, 30, 10)) # samples, timesteps, input_features
for (i in 1:1000) {
for (t in 1:30) {
x_train[i, t, ] = rnorm(10)
}
}
y_train = rnorm(1000)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 30)
x_train = array(data = 0, dim = c(1000, 30, 10)) # samples, timesteps, input_features
for (i in 1:1000) {
for (t in 1:30) {
x_train[i, t, ] = rnorm(10)
}
}
y_train = rnorm(1000)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 30)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 25,
validation_split = 0.3,
verbose = 1
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 25,
validation_split = 0.3,
verbose = 1
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1, return_sequences = TRUE)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 2)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
dim(x_train)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
summary(model)
x_train
dim(x_train)
samples = 1000
timesteps = 5
input_features = 1
x_train = array(data = 0, dim = c(samples, timesteps, input_features)) # samples, timesteps, input_features
for (i in 1:samples) {
for (t in 1:timesteps) {
x_train[i, t, ] = rnorm(10)
}
}
for (i in 1:samples) {
for (t in 1:timesteps) {
x_train[i, t, ] = rnorm(input_features)
}
}
y_train = rnorm(samples)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 2)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
input_features = 2
x_train = array(data = 0, dim = c(samples, timesteps, input_features)) # samples, timesteps, input_features
for (i in 1:samples) {
for (t in 1:timesteps) {
x_train[i, t, ] = rnorm(input_features)
}
}
y_train = rnorm(samples)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 2)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 2, input_shape = c(1, timesteps, input_features))
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 2, input_shape = c(1, timesteps, input_features))
model = keras_model_sequential() %>%
layer_simple_rnn(units = 2, input_shape = c(samples, timesteps, input_features))
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1)
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1, input_shape = c(1, 1))
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1, input_shape = c(1, 1,1))
model = keras_model_sequential() %>%
layer_simple_rnn(units = 1, input_shape = c(5, 2))
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
model = keras_model_sequential() %>%
layer_simple_rnn(units = 2, input_shape = c(5, 2))
model %>% compile(
optimizer = "rmsprop",
loss = list("mean_squared_error"),
metrics = list("mean_squared_error")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 10,
validation_split = 0.1,
verbose = 1
)
True
true
FALSE
as.double(FALSE)
as.double(TRUE)
choose(500, 5)
choose(500+5-1, 5)
choose(100+5-1, 5)
choose(100, 56)
cat(10)
